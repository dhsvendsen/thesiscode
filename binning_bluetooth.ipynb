{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime as dt # for ticking POSIX clock see http://www.ucolick.org/~sla/leapsecs/epochtime.html\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import graphviz\n",
    "import pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------- #\n",
    "## Handle raw data ##\n",
    "# ----------------- #\n",
    "\n",
    "def datetime_to_posix(year,month,day,hour=0,minute=0,second=0):\n",
    "    \"\"\"Return posix time, given datetime\n",
    "    \"\"\"\n",
    "    return ( time.mktime( dt.datetime(year,month,day,hour,minute,second).timetuple() ) )\n",
    "                       \n",
    "                       \n",
    "def time_bin_network(df, window=3600):\n",
    "    \"\"\"Return iterator to partition network in 'window'-sized bins.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : network dataframe\n",
    "        Pandas dataframe with network loaded straight from csv.\n",
    "    window : bin-size integer\n",
    "        Size of network bins in seconds. This number controls\n",
    "        the amount of bins.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_slice : iterator\n",
    "        Iterator object that yields one bin at a time.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        assert window >= 300\n",
    "        window += 0.01\n",
    "    except AssertionError:\n",
    "        raise AssertionError(\"\"\"'window' must be greater than, \\\n",
    "or equal to 300.\"\"\")\n",
    "        \n",
    "    # Get timestamp column and extreme values\n",
    "    col_t = df[\"timestamp\"]\n",
    "    min_t = min(col_t)\n",
    "    max_t = max(col_t)    \n",
    "    \n",
    "    # Get timespan and number of network splits - requires traversing data twice per split - good thing its \n",
    "    delta_time = max_t - min_t\n",
    "    \n",
    "    n_splits = int(math.ceil(delta_time/float(window)))\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        lower_bound = df[\"timestamp\"] > min_t + i*window\n",
    "        upper_bound = df[\"timestamp\"] < min_t + (i+1)*window\n",
    "        df_slice = df[lower_bound][upper_bound]\n",
    "        yield df_slice\n",
    "        \n",
    "        \n",
    "def dump_binned_network(df, binsize, filename, kind):\n",
    "    \"\"\"Calls binned_network and store binned network into local file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : network dataframe\n",
    "        Pandas dataframe with network\n",
    "    binsize : int\n",
    "        Size of bins in resulting network\n",
    "    filename : str\n",
    "        Name of resulting file\n",
    "    \"\"\"\n",
    "    \n",
    "    binned_network = list(time_bin_network(df, window=binsize))\n",
    "    \n",
    "    with open('../Data/processed_data/binned_networks/'+kind+'/'+filename+'.pickle', 'w') as outfile:\n",
    "        pickle.dump(binned_network, outfile)\n",
    "        \n",
    "            \n",
    "def load_binned_network(kind,filename):\n",
    "    with open('../Data/processed_data/binned_networks/'+kind+'/'+filename+'.pickle', 'r') as infile:\n",
    "        return pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- #\n",
    "## Reformat layered data ##\n",
    "# ----------------------- #\n",
    "\n",
    "def network_reformat_multiplex(layers, halflife=-1, w5min=1, norm_intra=\"False\", expmult=1, laydiff=10 ):\n",
    "    \"\"\"Return multiplex representation of multiplex network\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    halflife : number\n",
    "        Halflife in seconds of relax-rate decay between layers.\n",
    "        Defaults to -1.\n",
    "    layers : pandas df formatted layers\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    net_file : string\n",
    "        A network string in multiplex format\n",
    "    int_to_hash : dict\n",
    "        Key-value pairs of node integer id and original hash id\n",
    "    \"\"\"\n",
    "    \n",
    "    # Infomap will only work with node ids as indices.\n",
    "    \n",
    "    # Get all node ids in original md5 hash values\n",
    "    nodes = set()\n",
    "    for l, df in enumerate(layers):\n",
    "        layer_nodes = set()\n",
    "        layer_nodes.update(df[\"user1\"])\n",
    "        layer_nodes.update(df[\"user2\"])\n",
    "        nodes.update(layer_nodes)\n",
    "        \n",
    "    \n",
    "    ##########################\n",
    "    ## Add vertices to file ##\n",
    "    ##########################\n",
    "    \n",
    "    out_file = \"*Vertices %d\" % len(nodes)\n",
    "    \n",
    "    # Node name book-keeping, and adding to file\n",
    "    hashid_to_intid = {}\n",
    "    intid_to_hashid = {}\n",
    "    for i,n in enumerate(nodes):\n",
    "        intid = i+1\n",
    "        hashid = str(n)\n",
    "        out_file += '\\n%d \"Node %s\" 1.0' % (intid,hashid)\n",
    "        hashid_to_intid[hashid] = intid\n",
    "        intid_to_hashid[intid] = hashid\n",
    "\n",
    "        \n",
    "    #############################\n",
    "    ## Add Intra-edges to file ##\n",
    "    #############################\n",
    "    \n",
    "    out_file += \"\\n*Multiplex\\n# Intra edges: layer node layer node weight\"\n",
    "    \n",
    "    for l, df in enumerate(layers):\n",
    "        user1 = df[\"user1\"]\n",
    "        user2 = df[\"user2\"]\n",
    "            \n",
    "        edges = zip(user1, user2)\n",
    "        \n",
    "        # Add weights. REDUNDANT FOR 5MINS TIMESLICES BECAUSE PPL ONLY MEET ONCE HERE.\n",
    "        edges = [(e[0],e[1],w) for e,w in Counter(edges).items()]\n",
    "        \n",
    "        # Find max weight\n",
    "        maxw = max( [w for (a,b,w) in edges] )\n",
    "        \n",
    "        \n",
    "        # Add Intra-edges to file\n",
    "        for i,j,w in edges:\n",
    "            \n",
    "            if w==1:\n",
    "                w=w5min # reduce infomap confusion by random encounters\n",
    "            \n",
    "            if norm_intra:\n",
    "                w=w/float(maxw)\n",
    "                \n",
    "            out_file += '\\n%d %s %d %s %f' % (l+1,hashid_to_intid[i], l+1,hashid_to_intid[j],w) #+1 because 1 is first layer index\n",
    "        \n",
    "    #############################\n",
    "    ## Add Inter-edges to file ##\n",
    "    #############################\n",
    "    \n",
    "    out_file += \"# Inter edges: layer node layer node weight\"\n",
    "    \n",
    "    # Infinte halflife (represented as -1)\n",
    "    if halflife == -1:\n",
    "        return out_file, intid_to_hashid\n",
    "    \n",
    "    # Relax decay function\n",
    "    def N(t):\n",
    "        tau = halflife/np.log(2)\n",
    "        return expmult*np.exp(-t/float(tau))\n",
    "                        \n",
    "    for l1, df1 in enumerate(layers):\n",
    "        nodes1 = set(list(df1['user1'].values)+list(df1['user2'].values))\n",
    "        for l2, df2 in enumerate(layers):    \n",
    "            if not l2 > l1 or abs(l1-l2)>laydiff:\n",
    "                continue   \n",
    "            \n",
    "            nodes2 = set(list(df2['user1'].values)+list(df2['user2'].values))\n",
    "            common_nodes = nodes1 & nodes2\n",
    "            time_diff = df2['timestamp'].values[0] - df1['timestamp'].values[0]\n",
    "            \n",
    "            for n in common_nodes:\n",
    "                out_file += '\\n%d %s %d %s %f' % (l1+1,hashid_to_intid[n],l2+1,hashid_to_intid[n],N(time_diff))\n",
    "    \n",
    "    return out_file, intid_to_hashid\n",
    "\n",
    "\n",
    "def network_reformat_uniplex(layers):\n",
    "    \"\"\"Return graphs of timeslice-layer as a list of networkx graphs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    layers : pandas df formatted timeslice-layers\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    graphlist : list of networkx.classes.graph objects\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    #####################################################################\n",
    "    ## Make graphs by adding adding only nodes with edges between them ##\n",
    "    #####################################################################\n",
    "    \n",
    "    graphlist = []\n",
    "    \n",
    "    for l, df in enumerate(layers):\n",
    "        \n",
    "        #print( dt.datetime.fromtimestamp( min(df[\"timestamp\"]) ).isoweekday() )\n",
    "        \n",
    "        user1 = df[\"user1\"]\n",
    "        user2 = df[\"user2\"]\n",
    "        edges = zip(user1, user2)\n",
    "        \n",
    "        # Add weights. REDUNDANT FOR 5MINS TIMESLICES BECAUSE PPL ONLY MEET ONCE HERE.\n",
    "        edges = [(e[0],e[1],w) for e,w in Counter(edges).items()]\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        G.add_weighted_edges_from(edges)\n",
    "        graphlist.append(G)\n",
    "    \n",
    "    return graphlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/pandas/core/frame.py:1997: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"DataFrame index.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "### load raw data and cut specific month out\n",
    "#df_blue = pd.read_csv('../Data/raw_data/all.csv', sep=\" \").loc[:,['#','user1','user2','timestamp']]\n",
    "df_blue = pd.read_csv('../Data/raw_data/short.csv', sep=\" \").loc[:,['#','user1','user2','timestamp']]\n",
    "df_blue.columns = ['user1','user2','timestamp','duration']\n",
    "\n",
    "# feb14 starts 2014-02-01 00:00:00 and ends 2014-02-28 23:59:59\n",
    "startdate = datetime_to_posix(2014,2,1)\n",
    "enddate = datetime_to_posix(2014,2,28,23,59,59)\n",
    "feb14 = df_blue[ df_blue['timestamp'] > startdate ][ df_blue['timestamp'] < enddate ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user1</th>\n",
       "      <th>user2</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51bbc59b59c49e7af848d5172ede97</td>\n",
       "      <td>a86f60d62eaad69ef35e36650fc10b</td>\n",
       "      <td>1391209200</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2b8ead75b7a5ab690c49be08382131</td>\n",
       "      <td>3e003ad85a725fca24569f18f7fdb5</td>\n",
       "      <td>1391209200</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a94b118a62ebe7883a42d8966003b5</td>\n",
       "      <td>d224183095b2fab5f490b9c567a8fb</td>\n",
       "      <td>1391209200</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a5a21108856c86bbee025c40bdf05a</td>\n",
       "      <td>c71b8920ce278a5ad6d6da2b0f8df8</td>\n",
       "      <td>1391209200</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>688c883dc4b386437861c4333ecaa6</td>\n",
       "      <td>c71b8920ce278a5ad6d6da2b0f8df8</td>\n",
       "      <td>1391209200</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user1                           user2   timestamp  \\\n",
       "0  51bbc59b59c49e7af848d5172ede97  a86f60d62eaad69ef35e36650fc10b  1391209200   \n",
       "1  2b8ead75b7a5ab690c49be08382131  3e003ad85a725fca24569f18f7fdb5  1391209200   \n",
       "2  a94b118a62ebe7883a42d8966003b5  d224183095b2fab5f490b9c567a8fb  1391209200   \n",
       "3  a5a21108856c86bbee025c40bdf05a  c71b8920ce278a5ad6d6da2b0f8df8  1391209200   \n",
       "4  688c883dc4b386437861c4333ecaa6  c71b8920ce278a5ad6d6da2b0f8df8  1391209200   \n",
       "\n",
       "   duration  \n",
       "0       300  \n",
       "1       300  \n",
       "2       300  \n",
       "3       300  \n",
       "4       300  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_blue.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5mins\n",
      "15mins\n",
      "30mins\n",
      "hourly\n",
      "4hourly\n",
      "daily\n",
      "weekly\n"
     ]
    }
   ],
   "source": [
    "### Bin the selected month into timeslices of various lengths, and pickle for future use\n",
    "\n",
    "bins = [300, 900, 1800, 3600, 14400, 86400, 604800]\n",
    "bin_names = ['5mins', '15mins', '30mins', 'hourly', '4hourly', 'daily', 'weekly']\n",
    "\n",
    "for bs,bn in zip(bins,bin_names):\n",
    "    dump_binned_network( feb14 , bs , 'feb14' + '_' + bn, 'bluetooth' )\n",
    "    print(bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unpickling the pickles\n",
    "feb14_5mins = load_binned_network('bluetooth','feb14_5mins')\n",
    "feb14_15mins = load_binned_network('bluetooth','feb14_15mins')\n",
    "feb14_30mins = load_binned_network('bluetooth','feb14_30mins')\n",
    "feb14_hourly = load_binned_network('bluetooth','feb14_hourly')\n",
    "feb14_4hourly = load_binned_network('bluetooth','feb14_4hourly')\n",
    "feb14_daily = load_binned_network('bluetooth','feb14_daily')\n",
    "feb14_weekly = load_binned_network('bluetooth','feb14_weekly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Cut out a specific day for binsizes < 1 day\n",
    "\n",
    "bins_pr_day = {'5mins': 288, '15mins': 96, '30mins': 48, 'hourly': 24, '4hourly': 6 }\n",
    "weekstart = 9 # monday = 2\n",
    "datadays = 7 # how many days of data\n",
    "\n",
    "week1_5mins  = feb14_5mins[ bins_pr_day['5mins']*weekstart : bins_pr_day['5mins']*(weekstart + datadays)]\n",
    "week1_15mins = feb14_15mins[ bins_pr_day['15mins']*weekstart : bins_pr_day['15mins']*(weekstart + datadays)]\n",
    "week1_30mins = feb14_30mins[ bins_pr_day['30mins']*weekstart : bins_pr_day['30mins']*(weekstart + datadays)]\n",
    "week1_hourly = feb14_hourly[ bins_pr_day['hourly']*weekstart : bins_pr_day['hourly']*(weekstart + datadays)]\n",
    "week1_4hourly = feb14_4hourly[ bins_pr_day['4hourly']*weekstart : bins_pr_day['4hourly']*(weekstart + datadays)]\n",
    "week1_daily = feb14_daily[ weekstart : weekstart + datadays ]\n",
    "\n",
    "dt.datetime.fromtimestamp( min(week1_daily[0][\"timestamp\"]) ).isoweekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins_pr_day = {'5mins': 288, '15mins': 96, '30mins': 48, 'hourly': 24, '4hourly': 6 }\n",
    "weekstart = 2 # monday = 2\n",
    "datadays = 1 # how many days of data\n",
    "mon1_5mins  = feb14_5mins[ bins_pr_day['5mins']*weekstart : bins_pr_day['5mins']*(weekstart + datadays)]\n",
    "dt.datetime.fromtimestamp( min(mon1_5mins[0][\"timestamp\"]) ).isoweekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../Data/processed_data/binned_networks/bluetooth/mon1_5mins.pickle', 'w') as outfile:\n",
    "        pickle.dump(mon1_5mins, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Store the binned data as lists of networkx graphs to be visualized\n",
    "uniplex_network_filenames = [\"feb14_5min\", \"feb14_15min\", \"feb14_30min\",\n",
    "                             \"feb14_hourly\", \"feb14_4hourly\", \"feb14_daily\", \"feb14_weekly\"]\n",
    "layers_list = [feb14_5mins , feb14_15mins, feb14_30mins, feb14_hourly, feb14_4hourly, feb14_daily, feb14_weekly]\n",
    "\n",
    "#uniplex_network_filenames = [\"feb14_monday1_5min\", \"feb14_monday1_15min\", \"feb14_monday1_30min\",\n",
    "#                             \"feb14_monday1_hourly\", \"feb14_monday1_4hourly\", \"feb14_daily\", \"feb14_weekly\"]\n",
    "#layers_list = [oneday_5mins , oneday_15mins, oneday_30mins, oneday_hourly, oneday_4hourly, feb14_daily, feb14_weekly]\n",
    "\n",
    "#uniplex_network_filenames = [\"feb14_4hourly\"]                           \n",
    "#layers_list = [ feb14_4hourly ] \n",
    "               \n",
    "for filename, layers in zip(uniplex_network_filenames, layers_list) :\n",
    "    graphlist = network_reformat_uniplex( layers ) \n",
    "    with open('pickle_uniplex/bluetooth/'+filename+'.pickle', 'w') as outfile:\n",
    "        pickle.dump(graphlist, outfile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniplex_network_filenames = [\"mon1_5mins\",\"feb14_5min\"]\n",
    "layers_list = [mon1_5mins,feb14_5mins]\n",
    "\n",
    "for filename, layers in zip(uniplex_network_filenames, layers_list) :\n",
    "    graphlist = network_reformat_uniplex( layers ) \n",
    "    with open('pickle_uniplex/bluetooth/'+filename+'.pickle', 'w') as outfile:\n",
    "        pickle.dump(graphlist, outfile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.002956152\n"
     ]
    }
   ],
   "source": [
    "### Store binned data (layered data) as multiplex network in pajek .net format for infomap to use\n",
    "import timeit\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "\n",
    "muwhat=2.0552 # for SHORT feb14 5min bins #5.1675333 # for feb14 5min bins # 4.4557725 # for feb14 30min bins\n",
    "tau = 300#1800#14400#86400\n",
    "e=0.0001\n",
    "muwhatmults=np.arange(0.02,0.1001,0.02).tolist() + np.around(np.arange(0.15,1+e,0.05),3).tolist() + np.arange(1.5,4.5+e,0.5).tolist()\n",
    "\n",
    "for muwhatmult in muwhatmults:\n",
    "    netfile,intid_to_hashid = network_reformat_multiplex(mon1_5mins, halflife=tau, w5min=1, expmult=muwhatmult*muwhat)\n",
    "    multiplexname = \"mon1_5min_intra_norm_inter_2decay_{}muwhat\".format(muwhatmult)\n",
    "    with open(\"netfiles/bluetooth/\"+multiplexname+\".net\", 'w') as outfile:\n",
    "            outfile.write(netfile)\n",
    "    with open('int2hash/bluetooth/'+multiplexname+'.pickle', 'w') as outfile:\n",
    "            pickle.dump(intid_to_hashid, outfile)\n",
    "        \n",
    "print timeit.default_timer() - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "muwhat=2.0552 # for SHORT feb14 5min bins #5.1675333 # for feb14 5min bins # 4.4557725 # for feb14 30min bins\n",
    "tau = 300#1800#14400#86400\n",
    "muwhatmult=0.5\n",
    "netfile,intid_to_hashid = network_reformat_multiplex(feb14_5mins, halflife=tau, w5min=1, expmult=muwhatmult*muwhat)\n",
    "multiplexname = \"feb14_5min_intra_norm_inter_2decay_{}muwhat\".format(muwhatmult)\n",
    "with open(\"netfiles/bluetooth/\"+multiplexname+\".net\", 'w') as outfile:\n",
    "        outfile.write(netfile)\n",
    "with open('int2hash/bluetooth/'+multiplexname+'.pickle', 'w') as outfile:\n",
    "        pickle.dump(intid_to_hashid, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
